import pandas as pd
import numpy as np

import csv
from itertools import chain
import re
from tqdm import tqdm

import urllib.request as urllib2
import requests
import codecs
import warnings
from deep_translator import GoogleTranslator

from nltk import tokenize
import nltk


nltk.download('punkt')

def api_call(word, label, corpus, limit, sentence_len):
    # call api to get sentences
    word_url = url_encode(word)
    url = "https://www.dwds.de/r/?q=" + word_url + "&limit=" + str(limit) + "&corpus="+ corpus + "&view=tsv"
    df = pd.read_csv(url, sep='\t', encoding="utf-8")

    # if word does not exist in corpus
    if df.Hit.isnull()[0] == True:
        raise Exception("word does not exist in corpus")
        
    # only get sentences with n words
    df = df[df['Hit'].str.split().str.len().lt(sentence_len)]
    # delete sentences less than 2 words
    df = df[~df['Hit'].str.split().str.len().lt(2)]

    # split sentences into unnested lists
    df = df.Hit.map(tokenize.sent_tokenize)
    df = list(chain.from_iterable(df))

    # just get the sentence the word appears in
    df = [s for s in df if any(xs in s for xs in [word])]

    # make df with corresponding emotion
    df = pd.DataFrame({'label': len(df) * [label],
                       'text_de': df})
    
    df['corpus'] = corpus
    df['keyword'] = word
    
    return df 


def del_negation(df):
    df = df.dropna()
    df.reset_index(drop=True, inplace=True)
    df['text_de'] = df['text_de'].str.lower()
    
    df = df[~df['text_de'].str.contains('nicht')]
    df = df[~df['text_de'].str.contains('ohne')]
    df = df[~df['text_de'].str.contains('keine')]
    df = df[~df['text_de'].str.contains('kein')]
    df = df[~df['text_de'].str.contains('keinen')]
    df = df[~df['text_de'].str.contains('keinem')]
    df = df[~df['text_de'].str.contains('keiner')]
    df = df[~df['text_de'].str.contains('\?')]

    return df


def generate_example_text_based_on_keywords(word: str, label: str, limit=200, sentence_len=12):
    """
    A function used to generate sentences based on one (or more) words

    ...

    Attributes
    ----------
    word : str
        give a word(s) to get  the sentence the word(s) appears in

    corpus : str
        the used corpus of the API. (default=korpus21)

    label : str
        give a label to the word, which will be provided through output

    limit : int
        number of sentences being generated by the API (default=80)

    sentence_len : str
        length of sentence of a generated sentence by the API (default=12)
    """
    warnings.warn("Internetverbindung muss erhalten bleiben. No Error handling")
    
    corpus_list = ['korpus21', 'blogs', 'zeit', 'untertitel']
    
    df = [api_call(word, label, corpus, limit, sentence_len) for corpus in corpus_list]
    df = pd.DataFrame(df)
    df = pd.concat([i for i in df[0]], ignore_index=True, sort=False)
        
    if df.empty:
        df = df.append([np.nan], ignore_index=True)
        return df

    df = del_negation(df)
    
    df.drop_duplicates(subset=['text_de'], inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    return df

def url_encode(word):
    word = codecs.encode(word, 'utf-8')
    url_Word = urllib2.quote(word)

    return url_Word


def translate_texts(sentence):
    """
        A function used to translate sentences.
    """
    warnings.warn("Internetverbindung muss erhalten bleiben. No Error handling")

    return GoogleTranslator(source='en', target='de').translate(sentence)